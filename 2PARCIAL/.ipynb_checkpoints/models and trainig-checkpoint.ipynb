{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mdleiton/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mdleiton/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import collocations\n",
    "import re, string\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from scipy import sparse\n",
    "from sklearn import linear_model\n",
    "plt.rcParams['figure.figsize'] = [16, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getxy(row_s, row_f, feature_cols=['content',\"sin_stopwords\", 'followers', 'following', 'retweet',\"n_mentioned\",\"n_hashtags\"], label_col=['troll']):\n",
    "    return df[feature_cols][row_s:row_f], df[label_col][row_s:row_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mdleiton/Respaldo/repositorios/AED/env/lib/python3.5/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (2,3,4,8,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('alldataset_sentimental.csv', sep=\",\", encoding='utf-8')\n",
    "users_copy = df[df[\"troll\"] == False].copy()\n",
    "trolls_copy =  df[df[\"troll\"] == True].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>followers</th>\n",
       "      <th>following</th>\n",
       "      <th>retweet</th>\n",
       "      <th>created_at</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>n_mentioned</th>\n",
       "      <th>n_hashtags</th>\n",
       "      <th>user_mentioned</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>troll</th>\n",
       "      <th>emoticones</th>\n",
       "      <th>sin_stopwords</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>378147</th>\n",
       "      <td>grillo_marino</td>\n",
       "      <td>@leinmir @MadeleineOster3 @Politico_pe El trab...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>2019-08-07 23:32:38</td>\n",
       "      <td>leinmir</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>leinmir;MadeleineOster3;Politico_pe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trabajo congresistas miden solo cuantas leyes ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173519</th>\n",
       "      <td>pretty_juliette</td>\n",
       "      <td>RT @Jaimefmacias: Hoy tenemos doblete con @Jos...</td>\n",
       "      <td>941</td>\n",
       "      <td>1618</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-09-06 16:32:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jaimefmacias;JoseBauz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hoy doblete chile bolivia peru ecuador</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341692</th>\n",
       "      <td>GinoMayCry</td>\n",
       "      <td>RT @historyinmoment: Shanghai; China - 1990 vs...</td>\n",
       "      <td>961</td>\n",
       "      <td>1209</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-08-20 01:44:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>historyinmoment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shanghai china</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197839</th>\n",
       "      <td>JonathanJAST23</td>\n",
       "      <td>Hakuna Matata!! 游땙 @ Ciudadela Las Orquidias ht...</td>\n",
       "      <td>36</td>\n",
       "      <td>593</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-10-16 06:54:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>游땙</td>\n",
       "      <td>hakuna matata ciudadela orquidias</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365518</th>\n",
       "      <td>LaOrtecho</td>\n",
       "      <td>Quiero bajarme en S치enz Pe침a; sentarme en la b...</td>\n",
       "      <td>165</td>\n",
       "      <td>338</td>\n",
       "      <td>False</td>\n",
       "      <td>2019-04-29 12:56:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>quiero bajarme s치enz pe침a sentarme banca frent...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                            content  \\\n",
       "378147    grillo_marino  @leinmir @MadeleineOster3 @Politico_pe El trab...   \n",
       "173519  pretty_juliette  RT @Jaimefmacias: Hoy tenemos doblete con @Jos...   \n",
       "341692       GinoMayCry  RT @historyinmoment: Shanghai; China - 1990 vs...   \n",
       "197839   JonathanJAST23  Hakuna Matata!! 游땙 @ Ciudadela Las Orquidias ht...   \n",
       "365518        LaOrtecho  Quiero bajarme en S치enz Pe침a; sentarme en la b...   \n",
       "\n",
       "       followers following retweet           created_at reply_to  n_mentioned  \\\n",
       "378147        21        21   False  2019-08-07 23:32:38  leinmir          3.0   \n",
       "173519       941      1618    True  2016-09-06 16:32:26      NaN          2.0   \n",
       "341692       961      1209    True  2019-08-20 01:44:12      NaN          1.0   \n",
       "197839        36       593   False  2016-10-16 06:54:33      NaN          0.0   \n",
       "365518       165       338   False  2019-04-29 12:56:32      NaN          0.0   \n",
       "\n",
       "       n_hashtags                       user_mentioned hashtags  troll  \\\n",
       "378147          0  leinmir;MadeleineOster3;Politico_pe      NaN  False   \n",
       "173519          0                Jaimefmacias;JoseBauz      NaN  False   \n",
       "341692          0                      historyinmoment      NaN  False   \n",
       "197839          0                                  NaN      NaN  False   \n",
       "365518          0                                  NaN      NaN  False   \n",
       "\n",
       "       emoticones                                      sin_stopwords  polarity  \n",
       "378147        NaN  trabajo congresistas miden solo cuantas leyes ...       0.0  \n",
       "173519        NaN             hoy doblete chile bolivia peru ecuador       0.0  \n",
       "341692        NaN                                     shanghai china       0.0  \n",
       "197839          游땙                  hakuna matata ciudadela orquidias       0.0  \n",
       "365518        NaN  quiero bajarme s치enz pe침a sentarme banca frent...       0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395149, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['content', 'followers', 'following', 'retweet',\"n_mentioned\",\"troll\",'sin_stopwords',\"n_hashtags\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374753, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    183555\n",
       "True     127114\n",
       "True      38653\n",
       "False     25431\n",
       "Name: retweet, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['retweet'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    210906\n",
       "True     163847\n",
       "Name: troll, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['troll'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = getxy(0,299802)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = getxy(299803,316119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((299802, 7), (299802, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16316, 7), (16316, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = stopwords.words('spanish')\n",
    "stopwords_set.extend(stopwords.words('english'))\n",
    "stopwords_set = set(stopwords_set)\n",
    "vocab_size=5000\n",
    "tokenizer=feature_extraction.text.CountVectorizer(stop_words=stopwords_set, max_features=vocab_size)\n",
    "tokenizer=tokenizer.fit(df['sin_stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tok=tokenizer.transform(X_train['content'])\n",
    "X_test_tok=tokenizer.transform(X_test['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarize followers/following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train[['followers','following',\"n_mentioned\",\"n_hashtags\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rand means and scales: [5.36877281e+03 9.02260782e+02 1.34134862e+00 3.59674052e-01], [3.01723317e+04 2.77200829e+03 1.73109190e+00 1.19261143e+00]'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'rand means and scales: {}, {}'.format(scaler.mean_, scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_std = ['followers', 'following',\"n_mentioned\",\"n_hashtags\"]\n",
    "X_train[col_to_std]=scaler.transform(X_train[col_to_std])\n",
    "X_test[col_to_std]=scaler.transform(X_test[col_to_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers</th>\n",
       "      <th>following</th>\n",
       "      <th>n_mentioned</th>\n",
       "      <th>n_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208638</th>\n",
       "      <td>6.783408</td>\n",
       "      <td>10.333569</td>\n",
       "      <td>-0.197187</td>\n",
       "      <td>-0.301585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256981</th>\n",
       "      <td>-0.060777</td>\n",
       "      <td>-0.301680</td>\n",
       "      <td>-0.197187</td>\n",
       "      <td>0.536911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160521</th>\n",
       "      <td>-0.176910</td>\n",
       "      <td>-0.257669</td>\n",
       "      <td>-0.197187</td>\n",
       "      <td>-0.301585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172063</th>\n",
       "      <td>-0.146749</td>\n",
       "      <td>0.258202</td>\n",
       "      <td>-0.774857</td>\n",
       "      <td>-0.301585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167161</th>\n",
       "      <td>-0.177108</td>\n",
       "      <td>-0.283282</td>\n",
       "      <td>-0.197187</td>\n",
       "      <td>-0.301585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        followers  following  n_mentioned  n_hashtags\n",
       "208638   6.783408  10.333569    -0.197187   -0.301585\n",
       "256981  -0.060777  -0.301680    -0.197187    0.536911\n",
       "160521  -0.176910  -0.257669    -0.197187   -0.301585\n",
       "172063  -0.146749   0.258202    -0.774857   -0.301585\n",
       "167161  -0.177108  -0.283282    -0.197187   -0.301585"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[col_to_std].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>troll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208638</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256981</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160521</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172063</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167161</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        troll\n",
       "208638  False\n",
       "256981  False\n",
       "160521   True\n",
       "172063  False\n",
       "167161   True"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_to_bin = lambda x: 1 if x else 0\n",
    "y_train['troll'] = y_train['troll'].apply(bool_to_bin)\n",
    "y_test['troll'] = y_test['troll'].apply(bool_to_bin)\n",
    "\n",
    "# binarize retweet colum\n",
    "X_train['retweet'] = X_train['retweet'].apply(bool_to_bin)\n",
    "X_test['retweet'] = X_test['retweet'].apply(bool_to_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_features(tok_matrix, data_df):\n",
    "    \"\"\" concatenate the tokenized matrix (scipy.sparse) with other features \"\"\"\n",
    "    sparse_cols = sparse.csr_matrix(data_df[['followers', 'following', 'retweet',\"n_mentioned\",\"n_hashtags\"]])\n",
    "    combined = sparse.hstack([tok_matrix, sparse_cols])\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = concatenate_features(X_train_tok, X_train)\n",
    "X_test_combined = concatenate_features(X_test_tok, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299802, 5005)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model - Logistic Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mdleiton/Respaldo/repositorios/AED/env/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logic_model = linear_model.LogisticRegression().fit(X_train_combined, y_train['troll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855194428322693"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logic_model.score(X_train_combined, y_train['troll'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8494729100269673"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logic_model.score(X_test_combined, y_test['troll'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5005,)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_combined.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/mdleiton/Respaldo/repositorios/AED/env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/mdleiton/Respaldo/repositorios/AED/env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/mdleiton/Respaldo/repositorios/AED/env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/mdleiton/Respaldo/repositorios/AED/env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/mdleiton/Respaldo/repositorios/AED/env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()  # 5003 inputs\n",
    "model.add(Dense(1024, activation='relu', input_shape=X_train_combined.shape[1:])) # first layer\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(512, activation='relu'))  # second layer 1024 inputs\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(128, activation='relu'))  # third layer  512 inputs\n",
    "model.add(Dense(1, activation='sigmoid'))  # last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              5126144   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 5,716,737\n",
      "Trainable params: 5,716,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/mdleiton/Respaldo/repositorios/AED/env/lib/python3.5/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/mdleiton/Respaldo/repositorios/AED/env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/mdleiton/Respaldo/repositorios/AED/env/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting test scipy.sparse matrix to numpy\n",
    "X_test_matrix = X_test_combined.todense()\n",
    "X_train_matrix = X_train_combined.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisi칩n en el conjunto de prueba: 43.1049%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_matrix, y_test['troll'], verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print('Precisi칩n en el conjunto de prueba: %.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 239841 samples, validate on 59961 samples\n",
      "Epoch 1/30\n",
      "239841/239841 [==============================] - 60s 250us/step - loss: 0.2273 - acc: 0.9088 - val_loss: 0.2964 - val_acc: 0.8801\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.29638, saving model to trolls.model.best.hdf5\n",
      "Epoch 2/30\n",
      "239841/239841 [==============================] - 66s 277us/step - loss: 0.1943 - acc: 0.9231 - val_loss: 0.3117 - val_acc: 0.8801\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.29638\n",
      "Epoch 3/30\n",
      "239841/239841 [==============================] - 65s 273us/step - loss: 0.1701 - acc: 0.9335 - val_loss: 0.3262 - val_acc: 0.8805\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.29638\n",
      "Epoch 4/30\n",
      "239841/239841 [==============================] - 66s 276us/step - loss: 0.1514 - acc: 0.9413 - val_loss: 0.3399 - val_acc: 0.8794\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.29638\n",
      "Epoch 5/30\n",
      "239841/239841 [==============================] - 63s 264us/step - loss: 0.1393 - acc: 0.9461 - val_loss: 0.3455 - val_acc: 0.8823\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.29638\n",
      "Epoch 6/30\n",
      "239841/239841 [==============================] - 65s 271us/step - loss: 0.1281 - acc: 0.9509 - val_loss: 0.3679 - val_acc: 0.8807\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.29638\n",
      "Epoch 7/30\n",
      "239841/239841 [==============================] - 64s 266us/step - loss: 0.1194 - acc: 0.9545 - val_loss: 0.3816 - val_acc: 0.8791\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.29638\n",
      "Epoch 8/30\n",
      "239841/239841 [==============================] - 64s 266us/step - loss: 0.1134 - acc: 0.9571 - val_loss: 0.3838 - val_acc: 0.8806\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.29638\n",
      "Epoch 9/30\n",
      "239841/239841 [==============================] - 66s 277us/step - loss: 0.1080 - acc: 0.9588 - val_loss: 0.3975 - val_acc: 0.8816\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.29638\n",
      "Epoch 10/30\n",
      "239841/239841 [==============================] - 65s 270us/step - loss: 0.1040 - acc: 0.9602 - val_loss: 0.4018 - val_acc: 0.8818\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.29638\n",
      "Epoch 11/30\n",
      "239841/239841 [==============================] - 63s 264us/step - loss: 0.0987 - acc: 0.9626 - val_loss: 0.4250 - val_acc: 0.8807\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.29638\n",
      "Epoch 12/30\n",
      "239841/239841 [==============================] - 66s 273us/step - loss: 0.0952 - acc: 0.9634 - val_loss: 0.4401 - val_acc: 0.8807\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.29638\n",
      "Epoch 13/30\n",
      "239841/239841 [==============================] - 66s 275us/step - loss: 0.0932 - acc: 0.9644 - val_loss: 0.4336 - val_acc: 0.8819\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.29638\n",
      "Epoch 14/30\n",
      "239841/239841 [==============================] - 65s 271us/step - loss: 0.0884 - acc: 0.9666 - val_loss: 0.4414 - val_acc: 0.8810\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.29638\n",
      "Epoch 15/30\n",
      "239841/239841 [==============================] - 69s 287us/step - loss: 0.0874 - acc: 0.9668 - val_loss: 0.4671 - val_acc: 0.8818\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.29638\n",
      "Epoch 16/30\n",
      "239841/239841 [==============================] - 68s 282us/step - loss: 0.0853 - acc: 0.9678 - val_loss: 0.4517 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.29638\n",
      "Epoch 17/30\n",
      "239841/239841 [==============================] - 67s 278us/step - loss: 0.0820 - acc: 0.9688 - val_loss: 0.4570 - val_acc: 0.8801\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.29638\n",
      "Epoch 18/30\n",
      "239841/239841 [==============================] - 66s 273us/step - loss: 0.0812 - acc: 0.9692 - val_loss: 0.4560 - val_acc: 0.8805\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.29638\n",
      "Epoch 19/30\n",
      "239841/239841 [==============================] - 65s 272us/step - loss: 0.0791 - acc: 0.9699 - val_loss: 0.4883 - val_acc: 0.8800\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.29638\n",
      "Epoch 20/30\n",
      "239841/239841 [==============================] - 64s 266us/step - loss: 0.0774 - acc: 0.9706 - val_loss: 0.4776 - val_acc: 0.8818\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.29638\n",
      "Epoch 21/30\n",
      "239841/239841 [==============================] - 64s 267us/step - loss: 0.0761 - acc: 0.9706 - val_loss: 0.5013 - val_acc: 0.8802\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.29638\n",
      "Epoch 22/30\n",
      "239841/239841 [==============================] - 65s 269us/step - loss: 0.0738 - acc: 0.9714 - val_loss: 0.5005 - val_acc: 0.8806\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.29638\n",
      "Epoch 23/30\n",
      "239841/239841 [==============================] - 66s 275us/step - loss: 0.0733 - acc: 0.9721 - val_loss: 0.5157 - val_acc: 0.8799\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.29638\n",
      "Epoch 24/30\n",
      "239841/239841 [==============================] - 64s 266us/step - loss: 0.0720 - acc: 0.9723 - val_loss: 0.5386 - val_acc: 0.8797\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.29638\n",
      "Epoch 25/30\n",
      "239841/239841 [==============================] - 67s 278us/step - loss: 0.0713 - acc: 0.9727 - val_loss: 0.5304 - val_acc: 0.8796\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.29638\n",
      "Epoch 26/30\n",
      "239841/239841 [==============================] - 67s 280us/step - loss: 0.0695 - acc: 0.9732 - val_loss: 0.5329 - val_acc: 0.8811\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.29638\n",
      "Epoch 27/30\n",
      "239841/239841 [==============================] - 65s 269us/step - loss: 0.0697 - acc: 0.9733 - val_loss: 0.5416 - val_acc: 0.8819\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.29638\n",
      "Epoch 28/30\n",
      "239841/239841 [==============================] - 66s 276us/step - loss: 0.0680 - acc: 0.9737 - val_loss: 0.5373 - val_acc: 0.8805\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.29638\n",
      "Epoch 29/30\n",
      "239841/239841 [==============================] - 65s 272us/step - loss: 0.0671 - acc: 0.9739 - val_loss: 0.5468 - val_acc: 0.8809\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.29638\n",
      "Epoch 30/30\n",
      "239841/239841 [==============================] - 64s 269us/step - loss: 0.0677 - acc: 0.9736 - val_loss: 0.5337 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.29638\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='trolls.model.best.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(X_train_matrix, y_train['troll'], batch_size=1024, epochs=30, validation_split=0.2, callbacks=[checkpointer], verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisi칩n durante la prueba: 87.4969%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('trolls.model.best.hdf5')\n",
    "score = model.evaluate(X_test_matrix, y_test['troll'], verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "# mostrar la precisi칩n en prubea\n",
    "print('Precisi칩n durante la prueba: %.4f%%' % accuracy)\n",
    "# save model and weight\n",
    "algorithm_name = \"LSTM\"\n",
    "model.save(\"model\" + algorithm_name + \".h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"model\" + algorithm_name + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model\" + algorithm_name + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
